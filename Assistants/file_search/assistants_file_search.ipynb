{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assistants File Search\n",
    "본 노트북에서는, [Azure OpenAI Assistants (preview)의 File Search 툴](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/file-search?tabs=python)을 이용하여 Azure OpenAI 모델 외부의 지식 (제품 정보, 사용자 제공 문서등)으로 Assistant를 보강하는 방법을 보여줍니다. OpenAI는 문서를 자동으로 구문 분석 및 청킹하고, 임베딩을 생성 및 저장하고, 벡터 및 키워드 검색을 모두 사용하여 관련 콘텐츠를 검색하여 사용자 쿼리에 답변합니다.\n",
    "\n",
    "Azure OpenAI 리소스가 사전에 생성되어 있어야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소요 시간\n",
    "\n",
    "이 노트북을 실행하는데는 15분 정도 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages\\\n",
    "%pip install requests openai~=1.30.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터\n",
    "Azure OpenAI의 리소스에서 아래의 설정 정보를 복사하여 업데이트 하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "azure_endpoint = os.getenv(\"AZURE_OAI_ENDPOINT\")\n",
    "aoai_api_key = os.getenv(\"AZURE_OAI_KEY\")\n",
    "deployment_name = os.getenv(\"AZURE_OAI_DEPLOYMENT\")\n",
    "api_version = \"2024-02-15-preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable File Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "        api_key = aoai_api_key,  \n",
    "        api_version=\"2024-05-01-preview\",\n",
    "        azure_endpoint = azure_endpoint\n",
    "        )\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Document Analyst Assistant\",\n",
    "    instructions=\"You are an expert role library analyst. Use your knowledge base to answer questions about roles.\",\n",
    "    model=deployment_name,  # Correct model name\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Search를 위한 File Uplaod\n",
    "파일에 액세스 하기 위해 File Search 툴은 Vector Store 객체를 사용합니다. 파일을 업로드하고 파일을 포함할 Vector Store를 만듭니다. Vector Store가 만들어지면 모든 컨텐츠 처리가 완료 되었는지 확인하기 위해 모든 파일이 \"in_progress\" 상태를 벗어날때까지 Polling 해야합니다. SDK는 업로드 및 Polling을 위한 도우미를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "  api_key = aoai_api_key,  \n",
    "  api_version=\"2024-05-01-preview\",\n",
    "  azure_endpoint = azure_endpoint\n",
    ")\n",
    "\n",
    "# Create a vector store called \"Role Library\"\n",
    "vector_store = client.beta.vector_stores.create(name=\"Role Library\")\n",
    " \n",
    "# Ready the files for upload to OpenAI\n",
    "file_paths = [\"file_directory/role_library.pdf\"]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    " \n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "# and poll the status of the file batch for completion.\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    " \n",
    "# You can print the status and the file counts of the batch to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assistant가 새로운 Vector Store를 사용하도록 업데이트\n",
    "업로드한 파일이 액세스될 수 있도록, Assistant의 \"tool_resources\"를 새로운 \"vector_store\" ID로 업데이트 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the assistant to use the new vector store\n",
    "assistant = client.beta.assistants.update(\n",
    "  assistant_id=assistant.id,\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thread 생성\n",
    "스레드에 메시지 첨부 파일로 파일을 첨부할 수도 있습니다. 이렇게 함으로써, 스레드에 연결된 다른 'vector_store\"가 생성되거나, 만약 이 스레드에 이미 Vector Store가 붙어있다면, 기존의 스레드에 파일을 첨부할 수도 있습니다. 이 스레드의 \"Run\"을 생성할 때, File Search 툴은 당신의 Assistant의 \"vector_store\"와 이 Thread의 \"vector_store\"에서 모두 쿼리 할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the user provided file to OpenAI\n",
    "message_file = client.files.create(\n",
    "  file=open(\"file_directory/role_library_01.pdf\", \"rb\"), purpose=\"assistants\"\n",
    ")\n",
    " \n",
    "# Create a thread and attach the file to the message\n",
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Summarize the CTO roles in the library.\",\n",
    "      # Attach the new file to the message.\n",
    "      \"attachments\": [\n",
    "        { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "      ],\n",
    "    }\n",
    "  ]\n",
    ")\n",
    " \n",
    "# The thread now has a vector store with that file in its tool resources.\n",
    "print(thread.tool_resources.file_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Store는 마지막 활성 상태 (Vector Store가 실행의 일부였던 마지막 시간으로 정의 됨)로부터 7일이라는 기본 만료 정책이 있는 메시지 첨부 파일을 사용하여 만들어집니다. 이 기본값은 벡터 스토리지 비용을 관리하는데 도움이 됩니다. 이러한 만료 정책은 언제든지 재정의 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run을 생성하고 결과를 확인\n",
    "Run을 생성하고, 모델이 File Search 툴을 이용하여 사용자 질의에 대한 응답을 제공하는지 관찰합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler, OpenAI\n",
    " \n",
    "class EventHandler(AssistantEventHandler):\n",
    "    @override\n",
    "    def on_text_created(self, text) -> None:\n",
    "        print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_message_done(self, message) -> None:\n",
    "        # print a citation to the file searched\n",
    "        message_content = message.content[0].text\n",
    "        annotations = message_content.annotations\n",
    "        citations = []\n",
    "        for index, annotation in enumerate(annotations):\n",
    "            message_content.value = message_content.value.replace(\n",
    "                annotation.text, f\"[{index}]\"\n",
    "            )\n",
    "            if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "                cited_file = client.files.retrieve(file_citation.file_id)\n",
    "                citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "        print(message_content.value)\n",
    "        print(\"\\n\".join(citations))\n",
    "\n",
    "\n",
    "# Then, we use the stream SDK helper\n",
    "# with the EventHandler class to create the Run\n",
    "# and stream the response.\n",
    "\n",
    "with client.beta.threads.runs.stream(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    instructions=\"Please help to find out a job and role definition.\",\n",
    "    event_handler=EventHandler(),\n",
    ") as stream:\n",
    "    stream.until_done()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
